//Exercício desenvolvido por mim

/*Convolutional Neural Network in Pythorch

Adapt the CNN example for MNIST digit classfication from Notebook 3A. Feel free to play around with the model architecture and see how the training time/performance changes, but to begin, try the following:

Image ->
convolution (32 3x3 filters) -> nonlinearity (ReLU) ->
convolution (32 3x3 filters) -> nonlinearity (ReLU) -> (2x2 max pool) ->
convolution (64 3x3 filters) -> nonlinearity (ReLU) ->
convolution (64 3x3 filters) -> nonlinearity (ReLU) -> (2x2 max pool) -> flatten -> fully connected (256 hidden units) -> nonlinearity (ReLU) ->
fully connected (10 hidden units) -> softmax

Note: The CNN model might take a while to train. Depending on your machine, you might expect this to take up to half an hour. If you see your validation performance start to plateau, you can kill the training.*/

import imageio
import matplotlib.pyplot as plt
%matplotlib inline
# Read image
im = imageio.imread("./Downloads/download.jpg") ##Downloads é o diretório onde está a foto e download é o nome da foto

print("Shape of the image tensor: {}".format(im.shape))
plt.imshow(im)

#Red channel
im_red = im[:,:,0]
print(im_red.shape)
plt.imshow(im_red, cmap='gray')

#Blue channel
im_blue = im[:,:,2]
print(im_red.shape)
plt.imshow(im_blue, cmap = 'gray')

import numpy as np
import torch
import torch.nn.functional as F

#Create a random flat input vector
x_disney = torch.randn(100,784)

#Create weight matrix variable
W = torch.randn(784,10)/np.sqrt(784)
W.requires_grad_()

#Create bias variable
b = torch.zeros(10, requires_grad=True)

#Apply fully connected layer
y_disney = torch.matmul(x_disney, W) + b
y = F.relu(y_disney)

#Print input/output shape
print("Input shape: {}".format(x_disney.shape))
print("Output shape: {}".format(y.shape))

#Create a random image input tensor
x_disney1 = torch.randn(100,1,28,28)

#Create convolutional kernel variable
W1 = torch.randn(15,1,3,3)/np.sqrt(1*3*3)
W1.requires_grad_()

#Create bias variable
b1 = torch.zeros(15, requires_grad=True)

#Apply convolutional layer
conv1_disney = F.conv2d(x_disney1, W1, bias=b1, stride=1, padding=1) ##padding = redução do tamanho proporcionalmente ex. 6x6 para 3x3
conv1 = F.relu(conv1_disney)

#Print input/output shape
print("Input shape: {}".format(x_disney.shape))
print("Convolutional output shape: {}".format(conv1.shape))

#2nd Layer variables
W2 = torch.randn(32,15,3,3)/np.sqrt(15*3*3)
W2.requires_grad_()
b2 = torch.zeros(32, requires_grad=True)

#Apply 2nd convoluational layer
conv2 = F.relu(F.conv2d(conv1, W2, bias=b2, stride=1, padding=1))

#Print output shape
print("Second convolution output shape: {}".format(conv2.shape))

M = torch.zeros(4,3)

M2 = M.view(1,1,12)
M3 = M.view(2,1,2,3)
M4 = M.view(-1,2,3)
M5 = M.view(-1)

#Reshape flat input image into a 4d batched image input
x_flat = torch.rand(100,784)
x_reshaped = x_flat.view(-1,1,28,28)

#Print input shape
print(x_reshaped.shape)

#Flatten convolutional feature maps into a vector
h_flat = conv2.view(-1,28*28*32)

#Print output shape
print(h_flat.shape)

#Taking the output we've been working with so far, first print its current size
print("Shape of conv2 feature maps before pooling: {0}".format(conv2.shape))

#Max pool and the print new shape
max_pool2 = F.max_pool2d(conv2, kernel_size= 2)
print("Shape of conv2 feature maps after max pooling: {0}".format(max_pool2.shape))

#Average pool and then print new shape
avg_pool2 = F.avg_pool2d(conv2, kernel_size=2)
print("Shape of conv2 feature maps after avg pooling: {0}".format(avg_pool2.shape))

#Recreate values in pooling figure and make it 4d
feature_map_fig = torch.tensor(np.array([[1,1,2,4],[5,6,7,8],[3,2,1,0],[1,2,3,4]], dtype=np.float32))

fmap_fig = feature_map_fig.view(1,1,4,4)
print("feature map shape pre-pooling: {}".format(fmap_fig.shape))

#Maxpool
max_pool_fig = F.max_pool2d(fmap_fig, kernel_size=2)
print("\nMax pool")
print("Shape: {}".format(max_pool_fig.shape))
print(torch.squeeze(max_pool_fig))

#Avgpool
avg_pool_fig = F.avg_pool2d(fmap_fig, kernel_size=2)
print("\nAvg pool")
print("Shape: {}".format(avg_pool_fig.shape))
print(torch.squeeze(avg_pool_fig))

#Since striding is part of the convolution operation, we'll start with the feature maps before the 2nd convolution
print("Shape of conv1 feature maps: {0}".format(conv1.shape))

# Apply 2nd convolutional layer, with striding of 2
conv2_strided = F.relu(F.conv2d(conv1, W2, bias=b2, stride=2, padding=1))

# Print output shape
print("Shape of conv2 feature maps with stride of 2: {0}".format(conv2_strided.shape))

import torch.nn as nn

class MNIST_CNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)
        self.conv2 = nn.Conv2d(32, 32, kernel_size=3)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=3)
        self.conv4 = nn.Conv2d(64, 64, kernel_size=3)
        
        self.fc1 = nn.Linear(4*4*64, 256)
        self.fc2 = nn.Linear(256, 10)

    def forward(self, x):
        # conv layer 1
        x = self.conv1(x)
        x = F.relu(x)
                
        # conv layer 2
        x = self.conv2(x)
        x = F.relu(x)
        x = F.max_pool2d(x, kernel_size=2)
        
        # conv layer 3
        x = self.conv3(x)
        x = F.relu(x)
                
        # conv layer 4
        x = self.conv4(x)
        x = F.relu(x)
        x = F.max_pool2d(x, kernel_size=2)
                
       # fc layer 1
        x = x.view(-1, 4*4*64)
        x = self.fc1(x)
        x = F.relu(x)
        
        # fc layer 2
        x = self.fc2(x) 
        softmax = nn.Softmax(dim=1)
        x = softmax(x)
        
        return x    

model = MNIST_CNN()
print(model)

import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import datasets, transforms
from tqdm.notebook import tqdm, trange

# Load the data
mnist_train = datasets.MNIST(root="./datasets", train=True, transform=transforms.ToTensor(), download=True)
mnist_test = datasets.MNIST(root="./datasets", train=False, transform=transforms.ToTensor(), download=True)
train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=100, shuffle=True)
test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=100, shuffle=False)

## Training
# Instantiate model  
model = MNIST_CNN()  # <---- change here

# Loss and Optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # <---- change here

# Iterate through train set minibatchs 
for epoch in trange(3):  # <---- change here
    for images, labels in tqdm(train_loader):
        # Zero out the gradients
        optimizer.zero_grad()

        # Forward pass
        x = images  # <---- change here 
        y = model(x)
        loss = criterion(y, labels)
        # Backward pass
        loss.backward()
        optimizer.step()

## Testing
correct = 0
total = len(mnist_test)

with torch.no_grad():
    # Iterate through test set minibatchs 
    for images, labels in tqdm(test_loader):
        # Forward pass
        x = images  # <---- change here 
        y = model(x)

        predictions = torch.argmax(y, dim=1)
        correct += torch.sum((predictions == labels).float())

print('Test accuracy: {}'.format(correct/total))
